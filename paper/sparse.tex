\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{color}
\usepackage{array}
\usepackage{graphicx}
\usepackage{dblfloatfix}
\usepackage{amssymb}
\newcolumntype{?}{!{\vrule width 2pt}}


\title{Extending SpArSe: Automatic Gesture Recognition Architectures for Embedded Devices}
\author{J. Borrego-Carazo, E. Buscherm\"ohle, D. Castells-Rufas, J. Carrabina, E. Biempica, C. M\"uller}
\date{}

\begin{document}
\maketitle
\abstract
SpArSe
\begin{itemize}
	\item http://ictai2020.org/index.html : 10 Junio. Comunican 16 Agosto Rango CORE B
	\item http://islab.di.uminho.pt/ideal2020/ : 5 Junio. COmunican 10 Julio Rango CORE C
	\item https://mike2020.sigappfr.org/: 29 Junio COmunican 24 agosto NO RANGO
\end{itemize}
\section{Introduction}

Traditionally, optimization and tuning of machine learning model hyper-parameters was carried by means of expert knowledge, grid search or random search \cite{Bergstra2012}. However, due to the long evaluation times of machine learning models, specially neural networks, it was costly and inefficient to implement the two latter strategies in terms of capital, energy and time (\cite{Zoph2016}, \cite{Zoph2018}).

An alternative is to use Bayesian Optimization \cite{Mockus1978} for performing a result oriented search of the hyper-parameter space. There are two main components of a Bayesian Optimization framework \cite{Frazier2018}: the surrogate function, which models the objective function (i.e. the performance of the original model), and the acquisition function, which provides the next sampling points. Traditional surrogate models are Gaussian processes or decision trees. However, gaussian process based Bayesian optimization is not directly suited for conditional spaces. To enable this possibility, new kernels were developed \cite{Swersky2014}.

Additionally and as a parallel problem, the training and use of deep neural networks was computationally demanding. In order to make neural networks more efficient and also to be able to embed them into resource constrained devices several strategies have been developed by researchers: pruning (\cite{Han2015}, \cite{Cun:1990:OBD:109230.109298}), quantization \cite{Jacob2017} and efficient operations \cite{Howard2017}, among others. Along these improvements new software tools have begun to appear to port networks to resource constrained environments (\cite{TFLite}, \cite{Rotem2018}, \cite{ARMNN}, \cite{TensorRT}, \cite{uTensor}).

However, these procedures are focused in modifying an already trained structure and avoiding huge drops in performance; the neural architecture is not modified. The Neural Architecture Search (NAS) was a different problem: it focused on finding best performing architectures by modifying the network itself, usually by means of an evolutionary or genetic algorithm \cite{K.O.2002}. Lately, NAS algorithms have focused their attention into developing architectures which are both well performing but constrained in resource consumption and usage \cite{Elsken2018}. However, although delivering state of the art performance and efficiency, these methods still require high computational resources.

From these three problems, hyper-parameter optimization, NAS and making NNs resource efficient, stems off SpArSe \cite{Fedorov2019}. The main objective is not only building state of the art performance CNNs, but also managing both the size of the model and feature maps. While controlling the size of the model allows to work with resource constrained devices with limited ROM, controlling the size of the feature maps allows to adapt to different RAM memory sizes. It is built upon four main components. First, the hyperparameter search space, which defines the possible network topologies. Second, Multi-task Bayesian Optimization is used to pursue three objectives: performance, working size and working memory. Third, pruning is used to push the reduction of used parameters. And, finally, morphisms of the original and subsequent networks are used to avoid random initializations and costly Gaussian process fitting.

Altogether with SpArSe, there have been more efforts centered in providing NAS solutions for resource constrained environments (\cite{Loni2020}, \cite{Cai2019f}, \cite{Li2018n}, \cite{Lu2019}). However, most of these efforts center their attention in CNNs and do not include RNNs, and when they include them \cite{Pham2018}, the focus is not centered in minimizing the memory and size footprint of the architectures found. This lack, altogether with the explosion of new technologies and environments enabling or demanding gesture recognition (Virtual reality, smart cars, Kinekt sensors, wearables, among others), establishes an excellent opportunity for automatizing the building of gesture recognition networks for embedded environments.

In this work, we extend the purpose and usage of Sparse to recurrent neural networks and combinations with convolutional neural networks. We also include latency as a new objective, for tasks which have a time constraint on performance. With this we are able to develop gesture recognition solutions specially suited for embedded devices and time constrained tasks. We apply our implementation in a popular gesture classification dataset, the Corpus of Social Touch (CoST \cite{Jung2014}). In addition, we modify the search procedure introducing a low cost fidelity by running less epochs at the beginning and increasing them as the program advances. Hence, this forces the search algorithm to be more exploratory at the beginning and employ more time with Pareto solutions by the end of the procedure. Also, we include an analysis of the kernel employed to account for conditional spaces \cite{Swersky2014}, and another analysis for the importance of latency inclusion as an objective.

\section{Related work}

Regarding NAS algorithms, they are based on three components \cite{Elsken2019a}: search space, search strategy and performance  measure. 

The search space defines the allowed or accessible components for building the network. In most cases, the search restricts to CNNs and hence is focused primarily on images. However, modifications have already been made to account for other types of inputs, such as in the case of \cite{Wang2019a}.

Regarding the search strategy, there are three main approaches in the literature: evolutionary algorithms (EA) (\cite{Xie2017}, \cite{Stanley2002}), bayesian optimization \cite{Zhou2019a} and reinforcement learning (RL) \cite{Baker2016}. 

%%Using evolutionary algorithms to better design neural networks dates back to the end of XX century \cite{Branke2017}. The common procedure is to define each individual of the population as a network, while applying classical evolutionary procedures of crossover, mutation and selection according to a fitness measure. In the case of bayesian optimization, the procedure is to model the performance of the architecture with a surrogate model build upon the search space and new architectures are sampled using an acquisition function. The surrogate model is traditionally a tree or a gaussian process. In the case of reinforcement learning, the agent's activity consists in the building or extending, the architecture. The substantial difference among these methods is in which strategy thy use to optimize the policy. Finally, one-shot methods train a general network and select different networks comprised inside of it, thus allowing for a better resource usage. However, networks are biased and constrained towards the general network and the development is limited.

All those previously mentioned methods focus primary in one metric performance, usually the performance on the validation set according to the task metric. Lately, modifications of the following types of search strategies or new developments have been carried out. In \cite{Elsken2018} an evolutionary based NAS algorithm is implemented taking into account resource and outcome constraints. In \cite{Chu2019}, authors use a combination of RL and EA and optimize to find suitable architectures both on the error rate and the latency (or FLOPS during inference). In \cite{Gordon2017} they shrink and expand a established network in order to improve accuracy but also account for latency.

However, all these procedures are not oriented specifically for heavily constrained devices, such as microcontrollers, and the architectures developed cannot be deployed in such platforms. Hence, efforts have been made to develop procedures to deliver constrained networks. One of such techniques is to set off from a predefined network and conduct an optimization procedure for delivering a reduced but performing version (\cite{Loni2020}, \cite{Cai2019f}). Other techniques, such as in SpArSe \cite{Fedorov2019}, define a constrained search space and define different objectives both for ROM, RAM and accuracy.

%%Finally, an important aspect of NAS methods is its time cost, since most of them require evaluating several neural network architectures to perform optimizations. FOr this purpose, four main modifications are applied to the original procedures \cite{Elsken2019a}: lower fidelities, learning curve extrapolation, morphisms and one-shot models.

\section{SpArSe Modifications}\label{Sparsemod}

SpArSe is built upon two major components: the search space and the search strategy or procedure. As performance, it has a three objective target: accuracy, Model Size and RAM memory.

The search space, which is detailed in the original implementation, consists in sequential convolutional layers organized in blocks altogether with the possibility of branching through fully connected layers, as detailed in Figure \ref{fig:searchspaceoriginal}. It also includes pruning parameters. In our case we have deleted fully connected layers since they add more complexity and certainly make the network heavier. Although, as detailed in other studies \cite{Frankle2019}, this could help discovering new structures inside the network, in this case the internal structure would have to depend on this fully connected branching, which adds more weight without explicit improvement. Regarding the pruning, the original paper uses structured and structured pruning, while we have used only unstructured pruning.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/search_space_original.png}
	\caption{Original search Space made primarily with Convolution Blocks consisiting of several layers each, fully connected layers as branches and as final step before classification. Our modified space is exactly the same with the exception of branches.}
	\label{fig:searchspaceoriginal}
\end{figure}

The search procedure in the original implementation is based in three stages: in the first, new networks are sampled randomly or morphed from a previous one. In the second stage, the morphs are restricted to pruning. In the third step, the reference configurations are restricted optimal Pareto Points. The sampling process of architectures is through Thompson Sampling (TS). In general we conserve the three stage procedure, but with changes with regards sampling and model training. In the first stage, we follow a low fidelity scheme, as detailed in \cite{Elsken2019b}, consisting in training for only one epoch and sample models following a Sobol random procedure. In this case there are no morphisms, since we want to explore the space and obtain architectural performance information. In the second stage, we continue without morphisms but the sampling procedure is led by a Gaussian process (GP) with \cite{Swersky2014} as kernel and with Monte Carlo based Expected Improvement (EI) as acquisition function (AF). Thus, we rely in the AF for the proposal of new points, while in the original paper this was relegated to morphisms or TS. The reason is the evidence of greedy exploration by TS (\cite{Shahriari2016}, \cite{Shahriari2014}), while EI, among others, balances better the exploration/exploitation trade-off. We also increase  the epochs through which the models are trained. In the final stage, we base new sampling points only in morphisms directed through the AF and the training epochs are extended to a normal training procedure. An illustration of our search procedure is detailed at Figure \ref{fig:searchprocedure}.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{images/search_procedure.png}
	\caption{Left: Search procedure for our version of SpArSe. Right: original search procedure. In the original procedure, each configuration is either sampled randomly with probability $\rho$, or drawn as a morphism with probability $1 - \rho$. As far as the authors are aware, the GP is only in charge of fitting the data and enable the acquisition function to choose the best point from the sampled configurations.}
	\label{fig:searchprocedure}
\end{figure}

As an additional change, we have changed the RAM computation to the maximum value for the sum of parameters of input, output and weight along the network. That is, $MaxRAM = \|y_{l}\|_{0} + \|x_{l}\|_{0} + \|w_{l}\|_{0}$, where $l$ is the layer index, $y$ is the output of the operation, $x$ is the input and $w$ stands for the weight (including bias). In the original paper only input and output, or input and weight were included in working memory. The authors of the current work think that the three elements suppose a better approximation to the RAM computation in a real scenario.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{images/rnn_search_space.png}
	\caption{Different search space possibilities for the search space expansion of SpArSe. Overall they form a unique search space, however, there is the possibility to sample from each of them depending on the task. Left: a search space only composed of recurrent neural networks.Middle: a combination of 3D convolutions and recurrent networks. Right: a search space only for 3D convolutions.}
	\label{fig:rnnsearchspace}
\end{figure}

\section{Experimental Design}

First, we compare the original implementation with our implementation after modifications. The comparison is made with the MNIST and CIFAR2 \cite{Jose2013} datasets, with same preprocessing and splits as in the original paper.

\subsection{Sparse Extension and CoST Dataset}\label{Dataset}

The second step is to extend the SpArSe to gesture classification with temporal structures For this purpose, latency is included as another objective. We specify the latency of the network with the number of floating operations needed to predict a sample input. Then, we let the user specify the frequency or speed of its platform to finally obtain a latency measure. To be able to capture temporal dependencies, RNNs, GRU and LSTM, are also incorporated into the search space, as detailed in Figure \ref{fig:rnnsearchspace}. 

In order to test out the RNN and latency inclusion, and the overall suitability for gesture classification, the next task is to trial the extended version of SpArSe with a suited dataset: the Corpus of Social Touch \cite{Jung2014}. It consists of sequences coming from a pressure sensor arranged in a $8x8$ grid. Each of the values of the sensor is between 0 and 1023 and the sampling frequency is 135 Hz. Hence, each sequence has dimension $Nx8x8$, where N is its length. The lengths of the dataset vary from 10 to 1747 samples. The dataset was recorded with the participation of 31 subjects and comprises 14 classes: grab, hit, massage, pat, pinch, poke, press, rub, scratch, slap, stroke, squeeze, tap, and, tickle. In Figure \ref{fig:costsample}, a sample from the dataset averaged over channels is presented.

\subsubsection{Dataset Split and Preparation}\label{SectionDataSplit}

Regarding the dataset split there are inconsistencies among the papers regarding the data division. For example, \cite{Hughes2017} and \cite{Jung2015} divide the dataset randomly into 21 subjects for the training set and 10 for the test set.  The authors of the present study have not been able to find the specific ID of this partition. \cite{Ta2015a} seems to follow the same structure since the authors participated in the original competition. However, in \cite{Albawi2018} and \cite{Jung2017}, they use leave-one-subject-out cross validation for showing main results. In the present study, the dataset has been divided in two different manners. For the NAS procedure, the dataset has been split randomly between 20 subjects for training, 5 for validation and 6 for test. After finding the best performing network, we apply leave one out subject cross validation to obtain the final results. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/Cost_sample}
	\caption{Example of CoST samples. Left: grab gesture. Right: tickle gesture. Values of rpessure are standardized by mean subtraction and standard deviation division.}
	\label{fig:costsample}
\end{figure}




As stated in the previous section, the lengths of the sequences are quite large. These induces problems both for RNN and CNN based architectures. For CNNs, saving and using whole sequences would be a problem for embedded deployment since we would be storing a really large array. For RNNS, in the training phase, problems such as vanishing gradients might occur, while in the prediction phase, there is a dichotomy: if the RNN cell prediction is faster than the sampling rate there is no problem for real time prediction. However, if it is slower you are forced to store all the data in RAM, which can be impossible for most microcontroller capabilities (a sequence of, for example, 500 samples, equals 125 KB, in 32 bit float numbers). Hence, we devise two strategies: first, partition the sequences and predict on the sub-sequences, as done in previous studies (\cite{Albawi2018}, \cite{Hughes2017}), and second, pick samples from inside the sequence. With the first method, the whole sequence is not processed by the model, while in the second, although intermediate information is lost, the whole sequence is perceived. This is important since for some sequences the gesture is recorded at the end \cite{Ta2015a}. Both the number of chops or the sampling are included in the search procedure. 

The only preprocessing applied to the dataset is standardization by mean subtraction and standard deviation division.

\begin{table*}[tp]
	\center
	\begin{tabular}{|c ? c|c|c ? c|c|c|}
		\hline 
		& \multicolumn{3}{c?}{CIFAR-10 Binary} &  \multicolumn{3}{c|}{MNIST}\\
		\hline
		& Accuracy & Size & MaxRAM & Accuracy & Size & MaxRAM \\ 
		\hline
		DeepMaker \cite{Loni2020} & - & - & - & 95.9 & 507.81 & -  \\
		\hline 
		SpArSe \cite{Fedorov2019} &  73.84 & 0.78 & 1.28 &  96.49 & 1.44 & 1.33 \\ 
		\hline
		SpArSe Mod &  71.74  & 5.32 &  6.29 & 96.85  & 2.67 &  5.35 \\ 
		\hline 
	\end{tabular} 
	\caption{Results for the original version of SpArSe and our modified version. Accuracy is in $\%$ and Size corresponds to the model weight in KB taking into account only weights and not code. The maximum RAM is also in KB and corresponds, in our case, to the computation specified in Section \ref{Sparsemod}.}\label{ResultsSparse}
\end{table*}



\subsection{Kernel Analysis and Latency Proxy}\label{analysis}

The Arc Kernel \cite{Swersky2014} is defined as follows. Given an N dimensional space, $X$, with dimension bounds $[u_{i}, l_{i}] \in \Bbb{R}$, and some delta functions $ \delta_{i}: X_{i} \rightarrow \{True, False\}$, where, $i \in {1,...,D}$, an embedding $g_{i}: X_{i}\rightarrow \Bbb{R}^{2}$ is defined as:

\[
\begin{array}{ll}
[0, 0]^{T}& \mbox{if $\delta_{i}(\mathbf{x}) = false$} \\
 w_{i}[\sin{\pi\rho_{i}\frac{x_{i}}{u_{i}-l_{i}}},\cos{\pi\rho_{i}\frac{x_{i}}{u_{i}-l_{i}}}]^{T} & \mbox{otherwise}
 \end{array} 
\]

where $\omega_{i}\in\Bbb{R}^{+}$ and $\rho_{i}\in[0, 1]$. The, with a covariance function, for example the Radial Basis Function (RBF), we can define the Arc Kernel.


\begin{equation}
K(x, x) =  \prod_{i}\sigma^{2}\exp(-\frac{1}{2}d_{i}(\mathbf{x}, \mathbf{x'}))^{2}
\end{equation}

where $sigma$ is a scale parameters and $d_{i}=\|g_{i}(\mathbf{x})-g_{i}(\mathbf{x'})\|$

We can see that the capabilities for capturing conditionalities reside in the embeddding and the delta functions. As a side analysis of this work, we assay the performance of the Arc Kernel capturing conditional dependencies. For this purpose, we use a convolutional neural network with up to five layers with variable number of fiters targeted towards the Cifar10 dataset.

Finally, and also as an added study, we asses the utility of including latency as another objective. This test stems from the idea of model size being a proxy for latency \cite{Loni2020}. However, if this should be the case, a relationship between size and latency should be established to avoid including it as an objective but allowing the user to restrict the networks sampled.



%\textcolor{red}{CoST for \cite{Hughes2017}}: \textit{For CNNs and CNN-RNNs, the CoST data was split into windows with a window size of 45 samples (333ms) and a hop size of 15 samples (111ms).The convolutional neural networks used consisted of 3D convolutional layers and max pooling layers. Each convolu- tional layer extracts spatio-temporal features from the gesture window, ...\qquad Each gesture produces multiple measurement windows.
%The two RNN models provide a prediction of the gesture for each measurement window. To convert this to a single accuracy, the prediction of the class for the gesture capture as a whole was set as the most common prediction (i.e., majority voting) for the windows in the gesture capture. For the CNN model, we report the accuracy from the predictions of each window, rather than performing majority voting over the gesture as a whole. ....\qquad Additionally, for the CoST data, the duration of each capture is an informative features which was used by others [9], [10], and is unavailable to our approach. ... \qquad The memory requirements can be determined directly from each model: the number of values stored in RAM is the number of data points per sample window and total number of neurons in the model, and the number of values stored in FLASH is the number of weights and biases in the model....  \qquad we wish to maximize model accuracy while ensuring low memory requirements. Using additional layers, increasing the number of output channels, and decreasing the time dimension of each kernel in the CNN may improve accuracy, but would increase memory requirements, making the model unsuitable for in-material processing. Second, RNN layers suffer from an inability to capture long-term dependencies in a sequence; using long short-term memory (LSTM, [17]) layers could improve results, though at the expense of more parameters.... \qquad deep learning models explored here provide a touch gesture prediction in real-tine at a rate of 6 to 9 times per second,
%while the models in the Social Touch Challenge only provide
%predictions once the entire gesture is captured.}\\


%\textcolor{red}{CoST for \cite{Albawi2018}}:\textit{Another issue is the avoidance of preprocessing, which develops case dependency, and, as previously discussed, prevents real-time performance (e.g., using an average or any measurement which performs temporal abstraction}\\
%
%
%From other papers that also use pareto fronts it is to build a proxy function for evaluating the best of the best-> the pareto front draws a function which has an extreme point, we should use this point 


\section{Results}

The network builder and trainer has been developed using Pytorch, as well as the pruning and quantization procedures. The search algorithm is mainly build with Ax \cite{Bakshy2018a}, although some parts that needed more detail such as the Kernel have been implemented through BoTorch \cite{Balandat2019} and GPyTorch \cite{Gardner2018}. Each of the models and search procedures have been carried out with a single RTX 2070 Ti. 

\begin{figure}
	\centering
	\includegraphics[width=0.97\linewidth]{images/rnn_latency.png}
	\caption{Latency against Model Size for RNN Search Space. Model Size axis is logarithmic.  }
	\label{fig:latency}
\end{figure}

\subsection{SpArSe}

In Table \ref{ResultsSparse}, results for the modified version can be compared with the original implementation. Results for our version have similar accuracy performance and a little higher memory requirements. This could be both due to the difference in the computation procedure and the maximum RAM calculation method. Another possibility is the architecture change. The reason might be, following the findings in (\cite{Frankle2019}, \cite{Cai2019f}), the reduced search space and the impossibility of the pruning strategy to prune further compared to the original search space. That is, the bigger the search space the greater the probability of finding a well performing network with a smaller size and feature maps. 

A point that was not noted in the original paper is the utility of the Pareto frontier as final result. With the frontier we are able to choose among a range of different combinations suiting our direct hardware constraints. For example, and as illustrated in \ref{fig:paretocifar2}, we could choose the heavier network in the CIFAR2, with 74.55 $\%$ accuracy, 19.31 KB in size and 14.39 KB in RAM. Or, if we were really constrained for those resources, we could choose the other extreme with 57.83 $\%$ accuracy, 0.12 KB  in size and 1.44 KB in RAM. 


\begin{figure*}[bp]
	\centering
	\includegraphics[width=0.48\linewidth]{images/pareto_cifar2}
	\includegraphics[width=0.48\linewidth]{images/evolution_cifar2.png}
	\caption{Left: Pareto frontier for our implementation of Sparse and Cifar 10 Binary. Notice the different possibilities to choose among regarding RAM, size and performance. Each point corresponds to a specific configuration of the search space. Right: full search procedure for CIFAR-10 Binary with the modified SpArSe. The color degradation shows the progress of the search.}
	\label{fig:paretocifar2}
\end{figure*}



\subsection{CoST}
First, we have compared the results between splitting or insampling the data for a pure RNN model, as estabished in Section \ref{SectionDataSplit}. Results are illustrated in table \ref{CostTable} and compared to other implementations. As seen, cutting the signal delivers better results than sampling points from it, probably due to loss of signal information when sampling. As seen the RNN method provides state of the art results for all metrics, making SpArSe solutions directly usable in resource constrained environments while obtaining the maximum performance for the configured search space. 

\begin{table*}
\center
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	Model	& Model Type & Accuracy & Model Size (KB) & Max RAM (KB) & Latency (ms) \\
	\hline
	Albawi \textit{et al.} \cite{Albawi2018}  & CNN & 63.71  & -  & -  & - \\
	\hline
	Ta \textit{et al.} \cite{Ta2015a} & Random Forest & 60.8  & -  & - & -  \\
	\hline
	Hughes \textit{et al.} \cite{Hughes2017} & CNN + RNN & 52.86 &  73.05 & 25.25 & -  \\
	\hline
	Ours & RNN Cut & \textbf{65.12} & \textbf{15.77} & \textbf{13.86} & 15.68 \\
	\hline
	Ours & RNN Insample & 58.38 & 32.20 & 31.89 & \textbf{15.29} \\
	\hline
\end{tabular}
\caption{Results for the CoST dataset with hold one subject out cross validation, as in \cite{Albawi2018} and \cite{Jung2015}.}\label{CostTable}
\end{table*}

\subsection{Size as proxy for latency}

In the methodology, Section \ref{analysis}, the doubt was raised about size being a proxy for latency. In Figure \ref{fig:latency}, latency is plotted against model size in logarithmic scale. It seems to be a linear relationship after this transform, that is, $L = A\cdot \log{MS} + B$, with regression coefficient $R^{2} = 0.9493$. However, several concerns should be raised. Models developed are sequential and not complex and branchlike graphs, hence more observations for more complex inference procedures should be obtained. Moreover, search spaces employed are by construction small compared to other tasks not related to resource constrained devices. Hence, observations for bigger networks should be obtained. A final point, is that this relationship is space dependent even if it exists, hence latency has to be in anyway measured. For this reason, it is worth including it as an objective if the task is latency limited.




\subsection{Arc Kernel}

In Figure \ref{fig:paretocifar2}, one can observe the progress of the seach procedure with respect to accuracy and model size. However, the fundamental question is if the progress made is due to the capability of the GP and Kernel to model the conditional nature of the network or only due to the lower fidelities, that is, because we increase epochs with time. 

To clarify this, we have compared the Matern Kernel and  the Arc Kernel to a simple problem: a neural network with one to five convolutional layers and different neurons for each layer targeted to the CIFAR 10 dataset. In Figure \ref{fig:arc} we can observe the performance of both the Matern and Arc kernels. Arc Kernel seems to be more noisy although final performance is similar or even better than Matern. WIth regards the space complexity capture, Arc Kernel ends assuming the conditional structure of the space, due to the latter QQ plot fitting. However, it shows difficulties during the process, as observed by the vertical lines. 

\begin{figure*}
	\centering
\includegraphics[width=0.48\linewidth]{images/matern_vs_arc.png}
\includegraphics[width=0.48\linewidth]{images/arc_qq.png}
\caption{Left: evolution of performance of a CNN with variable layers in a bayesian optimization procedure with a GP based on Arc (blue) and Matern (red). Vertical bars indicate standard deviation of the different evaluations inside the same step. Right: Prediction values of the GP for each evaluation step and the real observed values (by evaluating the network)}
\label{fig:arc}
\end{figure*}


\section{Conclusions and further work}

In this study we have modified and extended the procedure and architecture introduced in \cite{Fedorov2019}. By adapting the search space, including 3DCNN and RNNS, we have been able to develop state of the art methods for a Gesture Recognition task. An analysis of the Arc Kernel \cite{Swersky2014} has also been presented, showcasing its similar performance to other available kernels. Finally, and as preliminary results, model size is determined as a proxy for latency. 
\bibliography{AEN42_ML_PHD_STUDIES, extra}
\bibliographystyle{ieeetr}
\end{document}