\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bergstra2012}
\citation{Zoph2016}
\citation{Zoph2018}
\citation{Mockus1978}
\citation{Frazier2018}
\citation{Swersky2014}
\citation{Han2015}
\citation{Cun:1990:OBD:109230.109298}
\citation{Jacob2017}
\citation{Howard2017}
\citation{TFLite}
\citation{Rotem2018}
\citation{ARMNN}
\citation{TensorRT}
\citation{uTensor}
\citation{K.O.2002}
\citation{Elsken2018}
\citation{Fedorov2019}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{Loni2020}
\citation{Cai2019f}
\citation{Li2018n}
\citation{Lu2019}
\citation{Pham2018}
\citation{Jung2014}
\citation{Swersky2014}
\citation{Elsken2019a}
\citation{Wang2019a}
\citation{Xie2017}
\citation{Stanley2002}
\citation{Zhou2019a}
\citation{Baker2016}
\citation{Elsken2018}
\citation{Chu2019}
\citation{Gordon2017}
\citation{Loni2020}
\citation{Cai2019f}
\citation{Fedorov2019}
\citation{Frankle2019}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}SpArSe Modifications}{2}{section.3}\protected@file@percent }
\newlabel{Sparsemod}{{3}{2}{SpArSe Modifications}{section.3}{}}
\citation{Elsken2019b}
\citation{Swersky2014}
\citation{Shahriari2016}
\citation{Shahriari2014}
\citation{Jose2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Original search Space made primarily with Convolution Blocks consisiting of several layers each, fully connected layers as branches and as final step before classification. Our modified space is exactly the same with the exception of branches.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:searchspaceoriginal}{{1}{3}{Original search Space made primarily with Convolution Blocks consisiting of several layers each, fully connected layers as branches and as final step before classification. Our modified space is exactly the same with the exception of branches}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Left: Search procedure for our version of SpArSe. Right: original search procedure. In the original procedure, each configuration is either sampled randomly with probability $\rho $, or drawn as a morphism with probability $1 - \rho $. As far as the authors are aware, the GP is only in charge of fitting the data and enable the acquisition function to choose the best point from the sampled configurations.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:searchprocedure}{{2}{3}{Left: Search procedure for our version of SpArSe. Right: original search procedure. In the original procedure, each configuration is either sampled randomly with probability $\rho $, or drawn as a morphism with probability $1 - \rho $. As far as the authors are aware, the GP is only in charge of fitting the data and enable the acquisition function to choose the best point from the sampled configurations}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Design}{3}{section.4}\protected@file@percent }
\citation{Jung2014}
\citation{Hughes2017}
\citation{Jung2015}
\citation{Ta2015a}
\citation{Albawi2018}
\citation{Jung2017}
\citation{Albawi2018}
\citation{Hughes2017}
\citation{Ta2015a}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Different search space possibilities for the search space expansion of SpArSe. Overall they form a unique search space, however, there is the possibility to sample from each of them depending on the task. Left: a search space only composed of recurrent neural networks.Middle: a combination of 3D convolutions and recurrent networks. Right: a search space only for 3D convolutions.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:rnnsearchspace}{{3}{4}{Different search space possibilities for the search space expansion of SpArSe. Overall they form a unique search space, however, there is the possibility to sample from each of them depending on the task. Left: a search space only composed of recurrent neural networks.Middle: a combination of 3D convolutions and recurrent networks. Right: a search space only for 3D convolutions}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Sparse Extension and CoST Dataset}{4}{subsection.4.1}\protected@file@percent }
\newlabel{Dataset}{{4.1}{4}{Sparse Extension and CoST Dataset}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of CoST samples. Left: grab gesture. Right: tickle gesture. Values of rpessure are standardized by mean subtraction and standard deviation division.}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:costsample}{{4}{4}{Example of CoST samples. Left: grab gesture. Right: tickle gesture. Values of rpessure are standardized by mean subtraction and standard deviation division}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Dataset Split and Preparation}{4}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{SectionDataSplit}{{4.1.1}{4}{Dataset Split and Preparation}{subsubsection.4.1.1}{}}
\citation{Loni2020}
\citation{Fedorov2019}
\citation{Swersky2014}
\citation{Loni2020}
\citation{Bakshy2018a}
\citation{Balandat2019}
\citation{Gardner2018}
\citation{Frankle2019}
\citation{Cai2019f}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Kernel Analysis and Latency Proxy}{5}{subsection.4.2}\protected@file@percent }
\newlabel{analysis}{{4.2}{5}{Kernel Analysis and Latency Proxy}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Latency against Model Size for RNN Search Space (black) and 2DCNN (blue), along regression lines (red) for each one. $R^{2}_{RNN} = 0.9490$, $R^{2}_{2DCNN} = 0.8247$. Model Size axis is logarithmic. }}{5}{figure.5}\protected@file@percent }
\newlabel{fig:latency}{{5}{5}{Latency against Model Size for RNN Search Space (black) and 2DCNN (blue), along regression lines (red) for each one. $R^{2}_{RNN} = 0.9490$, $R^{2}_{2DCNN} = 0.8247$. Model Size axis is logarithmic}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{5}{section.5}\protected@file@percent }
\citation{Albawi2018}
\citation{Ta2015a}
\citation{Hughes2017}
\citation{Albawi2018}
\citation{Jung2015}
\citation{Albawi2018}
\citation{Jung2015}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results for the original version of SpArSe and our modified version. Accuracy is in $\%$ and Size corresponds to the model weight in KB taking into account only weights and not code. The maximum RAM is also in KB and corresponds, in our case, to the computation specified in Section \ref  {Sparsemod}.}}{6}{table.1}\protected@file@percent }
\newlabel{ResultsSparse}{{1}{6}{Results for the original version of SpArSe and our modified version. Accuracy is in $\%$ and Size corresponds to the model weight in KB taking into account only weights and not code. The maximum RAM is also in KB and corresponds, in our case, to the computation specified in Section \ref {Sparsemod}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}SpArSe}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}CoST}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Size as proxy for latency}{6}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Arc Kernel}{6}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Left: Pareto frontier for our implementation of Sparse and Cifar 10 Binary. Notice the different possibilities to choose among regarding RAM, size and performance. Each point corresponds to a specific configuration of the search space. Right: full search procedure for CIFAR-10 Binary with the modified SpArSe. The color degradation shows the progress of the search.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:paretocifar2}{{6}{7}{Left: Pareto frontier for our implementation of Sparse and Cifar 10 Binary. Notice the different possibilities to choose among regarding RAM, size and performance. Each point corresponds to a specific configuration of the search space. Right: full search procedure for CIFAR-10 Binary with the modified SpArSe. The color degradation shows the progress of the search}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for the CoST dataset with hold one subject out cross validation, as in \cite  {Albawi2018} and \cite  {Jung2015}. * indicates that it has been estimated from the netowrk details in original .paper}}{7}{table.2}\protected@file@percent }
\newlabel{CostTable}{{2}{7}{Results for the CoST dataset with hold one subject out cross validation, as in \cite {Albawi2018} and \cite {Jung2015}. * indicates that it has been estimated from the netowrk details in original .paper}{table.2}{}}
\citation{Fedorov2019}
\citation{Swersky2014}
\bibdata{AEN42_ML_PHD_STUDIES,extra}
\bibcite{Bergstra2012}{1}
\bibcite{Zoph2016}{2}
\bibcite{Zoph2018}{3}
\bibcite{Mockus1978}{4}
\bibcite{Frazier2018}{5}
\bibcite{Swersky2014}{6}
\bibcite{Han2015}{7}
\bibcite{Cun:1990:OBD:109230.109298}{8}
\bibcite{Jacob2017}{9}
\bibcite{Howard2017}{10}
\bibcite{TFLite}{11}
\bibcite{Rotem2018}{12}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions and further work}{8}{section.6}\protected@file@percent }
\bibcite{ARMNN}{13}
\bibcite{TensorRT}{14}
\bibcite{uTensor}{15}
\bibcite{K.O.2002}{16}
\bibcite{Elsken2018}{17}
\bibcite{Fedorov2019}{18}
\bibcite{Loni2020}{19}
\bibcite{Cai2019f}{20}
\bibcite{Li2018n}{21}
\bibcite{Lu2019}{22}
\bibcite{Pham2018}{23}
\bibcite{Jung2014}{24}
\bibcite{Elsken2019a}{25}
\bibcite{Wang2019a}{26}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Left: evolution of performance of a CNN with variable layers in a bayesian optimization procedure with a GP based on Arc (blue) and Matern (red). Vertical bars indicate standard deviation of the different evaluations inside the same step. Right: Prediction values of the GP for each evaluation step and the real observed values (by evaluating the network)}}{9}{figure.7}\protected@file@percent }
\newlabel{fig:arc}{{7}{9}{Left: evolution of performance of a CNN with variable layers in a bayesian optimization procedure with a GP based on Arc (blue) and Matern (red). Vertical bars indicate standard deviation of the different evaluations inside the same step. Right: Prediction values of the GP for each evaluation step and the real observed values (by evaluating the network)}{figure.7}{}}
\bibcite{Xie2017}{27}
\bibcite{Stanley2002}{28}
\bibcite{Zhou2019a}{29}
\bibcite{Baker2016}{30}
\bibcite{Chu2019}{31}
\bibcite{Gordon2017}{32}
\bibcite{Frankle2019}{33}
\bibcite{Elsken2019b}{34}
\bibcite{Shahriari2016}{35}
\bibcite{Shahriari2014}{36}
\bibcite{Jose2013}{37}
\bibcite{Hughes2017}{38}
\bibcite{Jung2015}{39}
\bibcite{Ta2015a}{40}
\bibcite{Albawi2018}{41}
\bibcite{Jung2017}{42}
\bibcite{Bakshy2018a}{43}
\bibcite{Balandat2019}{44}
\bibcite{Gardner2018}{45}
\bibstyle{ieeetr}
